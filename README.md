# cs6035-assignment-7-solved
**TO GET THIS SOLUTION VISIT:** [CS6035 Assignment 7 Solved](https://www.ankitcodinghub.com/product/cs6035-machine-learning-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;114813&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;3&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (3 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;CS6035 Assignment 7 Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (3 votes)    </div>
    </div>
Learning Goals of this Project:

<ul>
<li>Learning Basic Pandas Dataframe Manipulations</li>
<li>Learning more about Machine Learning (ML) Classification models and how they are used in a Cybersecurity Context.</li>
<li>Learning about basic Data pipelines and Transformations</li>
<li>Learning how to write and use Unit Tests when developing Python code</li>
</ul>
Important Reference Materials:

<ul>
<li><a href="https://numpy.org/doc/">NumPy Documentation</a></li>
<li><a href="https://pandas.pydata.org/docs/">Pandas Documentation</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">Scikit-learn Documentation</a></li>
<li><a href="https://youtu.be/kYoQiAamIpQ">Introduction Video</a></li>
</ul>
BACKGROUND

Many of the Projects in CS6035 are focused on offensive security tasks which are very applicable to&nbsp;<a href="https://en.wikipedia.org/wiki/Red_team">Red Team</a>&nbsp;activities which many of us may associate with cybersecurity. This project will be more focused on defensive security tasks which are usually considered&nbsp;<a href="https://en.wikipedia.org/wiki/Blue_team_(computer_security)">Blue Team</a>&nbsp;activites that are done by many corporate teams.

Historically many defensive security professionals have investigated malicious activity/files/code to create patterns (often called signatures) that can be used to detect (and prevent) malicious activity/files/code when they see that pattern again. Historically this was a relatively effective way of preventing known malware from infecting systems but it does nothing to protect against novel attacks. As attackers became more sophisticated they learned to tweak (or simply encode) their malicious activity/files/code to avoid detection from these simple pattern matching detections.

With this background information it would be nice if a more general solution could give a score to activity/files/code that pass through corporate systems every day and tell the security team that while a certain pattern may not exactly fit a signature of known malicious activity/files/code it appears to be very similar to examples that were seen in the past that were malicious. Luckily Machine Learning models can do exactly that if provided with proper training data! Thus it is no surprise that one of the most powerful tools in the hands of defensive cybersecurity professionals is Machine Learning. Modern detection systems will usually use a combination of Machine Learning models and pattern matching (Regular Expressions) to detect and prevent malicious activity on networks and devices.

This project will focus on teaching the basic fundamentals of data analysis and building/testing your own ML models in python using the open source libraries Pandas and Scikit-Learn.

Cybersecurity Machine Learning Careers and Trends

Machine learning in cybersecurity is a growing field. The area was considered among top trends by&nbsp;<a href="https://www.mckinsey.com/capabilities/risk-and-resilience/our-insights/cybersecurity/cybersecurity-trends-looking-over-the-horizon">McKinsey</a>&nbsp;in 2022.

Additional Information

<ul>
<li><a href="https://www.crowdstrike.com/cybersecurity-101/machine-learning-cybersecurity/">ML in Cybersecurity – Crowdstrike</a></li>
<li><a href="https://www.ibm.com/security/artificial-intelligence">AI for Cybersecurity – IBM</a></li>
<li><a href="https://www.deloitte.com/global/en/our-thinking/insights/topics/technology-management/tech-trends/2022/future-of-cybersecurity-and-ai.html">Future of Cybersecurity and AI – Deloitte</a></li>
</ul>
&nbsp;

<h1>Frequently Asked Question(s) (FAQ)</h1>
<h3><strong>Getting Started</strong></h3>
<ul>
<li>Q: Are there any recommended documentation resources for Python libraries used on the project?**</li>
<li>A: The&nbsp;<a href="https://scikit-learn.org/stable/">scikit-learn documentation</a>is very useful for understanding how certain machine learning functions work and can serve as a valuable resource. The&nbsp;<a href="https://numpy.org/doc/">NumPy documentation can help with understanding common data structures and manipulation techniques used in data analysis. The&nbsp;</a><a href="https://pandas.pydata.org/docs/">Pandas documentation can help with understanding how to create and manipulate dataframes. Other sources may be useful as well.</a></li>
<li>Q: Are there any recommended video resources for Python libraries used on the project?**</li>
<li>A: YouTube can serve as an excellent source of learning for those who enjoy videos. One video that may be helpful to get a feel for machine learning concepts is&nbsp;<a href="https://www.youtube.com/watch?v=i_LwzRVP7bg">Machine Learning for Everybody – Full Course</a>created by freeCodeCamp.</li>
<li>Q: What general skills are needed to succeed on this project?</li>
</ul>
<ul>
<li>A:
<ul>
<li>Familiarity with Python programming environments and packaging:
<ul>
<li>Functions and the self keyword, parameters</li>
<li>Basic operators and loops</li>
<li>Basic understanding of NumPy and Pandas packages</li>
</ul>
</li>
<li>Familiarity with data science concepts:
<ul>
<li>Basic dataset preprocessing</li>
<li>Basic train/test splits</li>
<li>Basic implementation of Scikit learn modeling</li>
<li>Basic clustering and PCA</li>
</ul>
</li>
<li>High level understanding of data science algorithms:
<ul>
<li>Supervised learning models</li>
<li>Unsupervised learning models</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>High level understanding of model comparison metrics</li>
<li>Q: I am overwhelmed and don’t know where to start.**</li>
</ul>
<ul>
<li>A: Start simple with reviewing the useful links/videos we have provided and doing the coding tasks (tasks 1-5) in order. They will somewhat build on each other and will get progressively harder so early tasks are easier to complete.</li>
</ul>
<h3><strong>General Project Questions</strong></h3>
<ul>
<li>Q: When are office hours for this project?</li>
<li>A: There will be a pinned Ed Discussion Post with office hour date/times as well as recordings after they take place.</li>
<li>Q: Should I make my own post related to this project in Ed Discussion?**</li>
<li>A: Please try to ask your question in one of the pinned project posts and remove answer data (ie don’t post your code, even snippets) or other information that should not be publicly shared and ask in the public forum so others can benefit from your questions.</li>
<li>Q: Can you review my code in a private Ed Discussion Post?**</li>
<li>A: Since we have a Gradescope autograder we will not be reviewing code of students and expect you to debug your code using information in public posts in Ed Discussion or via google searches/stack overflow.</li>
<li>Q: I have constructive feedback that can improve this project for next semester**</li>
<li>A: Open a private Ed Discussion Post and we will review your ideas and may implement them in a future semester.</li>
</ul>
<h3><strong>Submission and Gradescope</strong></h3>
<ul>
<li>Q: How many submissions do we have in Gradescope?**</li>
<li>A: Unlimited</li>
<li>Q: I can’t see any scores/output in the autograder is it broken?**</li>
<li>A: We have a protection in the autograder to prevent printing sensitive information so if your code has print statements then you wont see your score or any outputs of the autograder. Please resubmit your code with print statements removed and you should see the normal outputs.</li>
<li>Q: I think I found a bug in the Autograder**</li>
<li>A: Open a private Ed Discussion Post and we can take a look. This is a relatively new project as CS6035 projects go so there is a chance that we missed something (edge cases) with how the autograder is checking vs how you coded your solution. If this happens we will make an update to the autograder to fix it and will make a pinned post letting students know the autograder was changed.</li>
</ul>
<h3><strong>Task Hints and Questions</strong></h3>
<ul>
<li>Q: I am using RFE to find the feature importance of a random forest or gradient boosting model and it is running for a long time and timing out in the autograder**</li>
<li>A: Only use RFE for logistic regression models and use the built in values of feature importance for random forest and gradient boosting models.</li>
</ul>
<h1>Setup</h1>
For this assignment if you want to run it locally we suggest that you use the following local setup instructions. You can install and run the packages with a variety of other software environments but you will need to figure out how to install and run the code in those environments yourself.

<h2>Anaconda Installation</h2>
First you should download anaconda from their website:&nbsp;<a href="https://www.anaconda.com/download">Anaconda Download</a>. It has installers for Windows, Linux and Mac so make sure you are downloading the right version then run the installation wizard.

For more information on how to install it see the following docs:

<ul>
<li><a href="https://docs.anaconda.com/free/anaconda/install/windows/">Installing on Windows</a></li>
<li><a href="https://docs.anaconda.com/free/anaconda/install/mac-os/">Installing on Mac</a></li>
<li><a href="https://docs.anaconda.com/free/anaconda/install/linux/">Installing on Linux</a></li>
</ul>
<h2>Environment Setup</h2>
Now that you have Anaconda installed we will set up the python environment.

<strong>Note:</strong>&nbsp;If you go the Pycharm route you can have it install your anaconda environment from the env.yml file we give you by following this&nbsp;<a href="https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html">guide</a>

<ol>
<li>Open up Anaconda Prompt</li>
<li>Download the&nbsp;Starter Code&nbsp;and&nbsp;Student Local Testing&nbsp;Folders from Canvas</li>
<li>Navigate to the Student Local Testing Folder</li>
<li>Inside that folder there is a&nbsp;env.yml&nbsp;file which Anaconda can use to install all the required packages. To install that environment run&nbsp;conda env create –file env.yml&nbsp;from the Anaconda Prompt once you are inside the&nbsp;Student Local Testing&nbsp;folder.</li>
<li>Anaconda should install your environment and from now on to activate it you can run&nbsp;conda activate cs6035_ML&nbsp;from inside Anaconda Prompt</li>
</ol>
<h2>Unit Test Setup</h2>
Before Running any Unit tests you need to copy the your edited Task Files that you are trying to locally test into the&nbsp;Student Local Testing&nbsp;Folder

Then Follow the directions for VS Code or Pycharm depending on which you have or decide to install on your machine.

<h3><strong>Visual Studio Code</strong></h3>
If you dont already have Visual Studio Code installed on your machine you can follow the install guide:

<ul>
<li><a href="https://code.visualstudio.com/docs/setup/windows">Installing on Windows</a></li>
<li><a href="https://code.visualstudio.com/docs/setup/mac">Installing on Mac</a></li>
<li><a href="https://code.visualstudio.com/docs/setup/linux">Installing on Linux</a></li>
</ul>
<a href="https://youtu.be/C6xqk-BA1I8?si=X5r_f64W5yBEXzPP">Local Testing Video</a>

Next you need to select the python from the anaconda environment we created&nbsp;cs6035_ML&nbsp;in VS Code.&nbsp;<a href="https://opensourceoptions.com/blog/setup-anaconda-python-to-work-with-visual-studio-code-on-windows/">Here is a guide</a>&nbsp;for doing so on windows if you dont have anaconda in your system’s PATH. VS Code also has some&nbsp;<a href="https://code.visualstudio.com/docs/python/environments">documentation</a>&nbsp;on environments if you are struggling.

To Setup Unit Testing:

<ol>
<li>Open up a VS Code window with the&nbsp;Student Local Testing&nbsp;Folder</li>
<li>Follow the&nbsp;<a href="https://code.visualstudio.com/docs/python/testing">testing docs</a>&nbsp;or basically just click the&nbsp;beaker&nbsp;shape on the left side bar and then&nbsp;Configure Python Tests</li>
<li>select&nbsp;unittest&nbsp;from the framework options</li>
<li>select&nbsp;tests&nbsp;as the directory containing the tests</li>
<li>select&nbsp;test_*.py&nbsp;as the apttern to identify test files</li>
</ol>
You should now see a drop down with the unit tests on the left hand sidebar and you can click the play button to run all the unit tests or each one individually. Tests that have a red x will show either an error or an incorrect answer while green checkmarks have passed the test case.

<h3><strong>Pycharm Community Edition</strong></h3>
If you dont have Pycharm installed on your machine you can follow the install guide:

<ul>
<li><a href="https://www.jetbrains.com/help/pycharm/installation-guide.html">Install Guide</a>&nbsp;We suggest installing the Community Edition (free version)</li>
</ul>
Once you have it installed use this&nbsp;<a href="https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html">guide</a>&nbsp;to setup your IDE with your anaconda environment. You can reference the instructions for Creating a conda environment based on environment.yaml .

Next you can follow the Pycharm Testing Docs to configure the test cases:

<ul>
<li><a href="https://www.jetbrains.com/help/pycharm/testing-your-first-python-application.html">Test your first Python application</a></li>
<li><a href="https://www.jetbrains.com/help/pycharm/testing.html">Testing Docs</a></li>
</ul>
The Test Sources Root for Pycharm should be the&nbsp;tests&nbsp;folder inside the&nbsp;Student Local Testing&nbsp;Folder.

<h2>Notebooks</h2>
You can use&nbsp;<a href="https://colab.research.google.com/">Google’s Colab</a>,&nbsp;<a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks">VS Code’s Notebooks</a>,&nbsp;<a href="https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html">Pycharm’s Notebooks</a>&nbsp;or&nbsp;<a href="https://jupyter.org/">Jupyter Notebooks</a>&nbsp;to write/debug your code for this assignment but we will not provide any support for this method. Ultimately you will still have to submit the Task.py to gradescope so make sure your code will run in those python files and not just in a notebook.

&nbsp;

<h1>Task 1 (15 points)</h1>
Lets first get familiar with some Pandas basics. Pandas is a library that handles dataframes which you can think of as a python class that handles tabular data. Generally in the real world you would also use plotting tools like PowerBi, Tableau, Data Studio, Matplotlib, etc., to create graphics and other visuals to better understand the dataset you are working with, this step is generally known as Exploratory Data Analysis. Since we are using an autograder for this class we will skip the plotting for this project. For this task we have released a test suite if you are struggling to understand the expected input and outputs for a function please set that up and use it to debug your function.

<h2>Useful Links:</h2>
<ul>
<li><a href="https://pandas.pydata.org/docs/">Pandas documentation — Pandas 1.5.3 documentation (pydata.org)</a></li>
<li><a href="https://www.ibm.com/topics/exploratory-data-analysis">What is Exploratory Data Analysis? – IBM</a></li>
<li><a href="https://www.kdnuggets.com/2020/05/top-10-data-visualization-tools-every-data-scientist.html">Top Data Visualization Tools – KDnuggets</a></li>
</ul>
<h2>Getting Started Video</h2>
<a href="https://youtu.be/DFDiDwIgXp8?si=ux2Yy9nyhmP_nU_r">Getting started with Notebooks and Functions Video</a>

<h2>Deliverables:</h2>
<ul>
<li>Complete the functions in task1.py</li>
<li>For this task we have released a local test suite please set that up and use it to debug your function.</li>
<li>Submit task1.py to gradescope</li>
</ul>
<h2>Instructions:</h2>
The Task1.py File has function skeletons that you will complete with python code (mostly using the pandas library). The Goal of each of these functions is to give you familiarity with the pandas library and some general python concepts like classes which you may not have seen before. See information about the Function’s Inputs,Outputs and Skeletons below

<h1>Table of contents</h1>
<ol>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#find_data_type">find_data_type</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#set_index_col">set_index_col</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#reset_index_col">reset_index_col</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#set_col_type">set_col_type</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#make_df_from_2d_array">make_DF_from_2d_array</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#sort_df_by_column">sort_df_by_column</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#drop_na_cols">drop_na_cols</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#make_new_column">make_new_column</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#left_merge_dfs_by_column">left_merge_dfs_by_column</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#simpleclass">simpleclass</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task1.html#find_dataset_statistics">find_dataset_statistics</a></li>
</ol>
<h2>find_data_type</h2>
In this function you will take a dataset and the name of a column in it and return what datatype the column is

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
data type of the column (np.dtype)

<h5><strong>Function Skeleton</strong></h5>
def find_data_type(dataset:pd.DataFrame,column_name:str) -&gt; np.dtype:&nbsp;&nbsp;&nbsp; return np.dtype()

<h2>set_index_col</h2>
In this function you will take a dataset and a series and set the index of the dataset to be the series

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>index&nbsp;– a pandas series that contains an index for the dataset</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe indexed by the given index series

<h5><strong>Function Skeleton</strong></h5>
def set_index_col(dataset:pd.DataFrame,index:pd.Series) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>reset_index_col</h2>
In this function you will take a dataset with an index already set and reindex the dataset from 0 to n-1, dropping the old index

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe indexed from 0 to n-1

<h5><strong>Function Skeleton</strong></h5>
def reset_index_col(dataset:pd.DataFrame) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>set_col_type</h2>
In this function you will be given a dataframe, column name and column type. You will edit the dataset to take the column name you are given and set it to be the type given in the input variable

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>column_name&nbsp;– a string containing the name of a column</li>
<li>new_col_type&nbsp;– a type to change the column to</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the column in&nbsp;column_name&nbsp;changed to the type in&nbsp;new_col_type

<h5><strong>Function Skeleton</strong></h5>
<em># Set astype (string, int, datetime)</em>def set_col_type(dataset:pd.DataFrame,column_name:str,new_col_type:type) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>make_DF_from_2d_array</h2>
In this function you will take data in an array as well as column and row labels and use that information to create a pandas dataframe

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>array_2d&nbsp;– a 2 dimensional numpy array of values</li>
<li>column_name_list&nbsp;– a list of strings holding column names</li>
<li>index&nbsp;– a pandas series holding the row index’s</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with columns set from&nbsp;column_name_list, row index set from&nbsp;index&nbsp;and data set from&nbsp;array_2d

<h5><strong>Function Skeleton</strong></h5>
<em># Take Matrix of numbers and make it into a dataframe with column name and index numbering</em>def make_DF_from_2d_array(array_2d:np.array,column_name_list:list[str],index:pd.Series) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>sort_DF_by_column</h2>
In this function you are given a dataset and column name and will return a sorted dataset (indexed from 0 to n-1) either in decending or ascending order depending on the value in the&nbsp;decending&nbsp;variable

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>column_name&nbsp;– a string that contains the column name to sort the data on</li>
<li>decending&nbsp;– a boolean value (True&nbsp;or&nbsp;False) for if the column should be sorted in decending order</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe sorted by the given column name and in decending or ascending order depending on the value of the&nbsp;decending&nbsp;variable

<h5><strong>Function Skeleton</strong></h5>
<em># Sort Dataframe by values</em>def sort_DF_by_column(dataset:pd.DataFrame,column_name:str,descending:bool) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>drop_NA_cols</h2>
In this function you are given a dataframe you will return a dataframe with any columns containing&nbsp;NA&nbsp;values dropped

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with any columns that contain an&nbsp;NA&nbsp;value dropped

<h5><strong>Function Skeleton</strong></h5>
<em># Drop NA values in dataframe Columns </em>def drop_NA_cols(dataset:pd.DataFrame) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>drop_NA_rows</h2>
In this function you are given a dataframe you will return a dataframe with any rows containing&nbsp;NA&nbsp;values dropped

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with any rows that contain an&nbsp;NA&nbsp;value dropped

<h5><strong>Function Skeleton</strong></h5>
def drop_NA_rows(dataset:pd.DataFrame) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>make_new_column</h2>
In this function you are given a dataset, new column name and a static value for the new column add the new column to the dataset and return the dataset

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/05_add_columns.html

<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>new_column_name&nbsp;– a string containing the name of the new column to be created</li>
<li>new_column_value&nbsp;– a string containing a static value that will be set for the new column for every row</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the new column created named&nbsp;new_column_name&nbsp;and filled with the value in&nbsp;new_column_value

<h5><strong>Function Skeleton</strong></h5>
def make_new_column(dataset:pd.DataFrame,new_column_name:str,new_column_value:str) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>left_merge_DFs_by_column</h2>
In this function you are given 2 datasets and the name of a column with which you will left join (left dataset is&nbsp;dataset1&nbsp;right dataset is&nbsp;dataset2) them on using the pandas merge method.

<h5><strong>Useful Resources</strong></h5>
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html https://stackoverflow.com/questions/53645882/pandas-merging-101

<h5><strong>INPUTS</strong></h5>
<ul>
<li>left_dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>right_dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>join_col_name&nbsp;– a string containing the column name to join the two dataframes on</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe containing the left 2 datasets left joined together

<h5><strong>Function Skeleton</strong></h5>
def left_merge_DFs_by_column(left_dataset:pd.DataFrame,right_dataset:pd.DataFrame,join_col_name:str) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; return pd.DataFrame()

<h2>simpleClass</h2>
This project will require you to work with Python Classes. If you are not familiar with them we suggest learning a bit more about them.

You will take the inputs into the Class initialization and set them as instance variables (of the same name) in the python class

<h5><strong>Useful Resources</strong></h5>
https://www.w3schools.com/python/python_classes.asp

<h5><strong>INPUTS</strong></h5>
<ul>
<li>length&nbsp;– an integer</li>
<li>width&nbsp;– an integer</li>
<li>height&nbsp;– an integer</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
None

<h5><strong>Function Skeleton</strong></h5>
class simpleClass():&nbsp;&nbsp;&nbsp; def __init__(self, length:int, width:int, height:int):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass

<h2>find_dataset_statistics</h2>
Now that you have learned a bit about pandas dataframes you can start using them to generate some simple summary statistics for a dataframe. You will be given the dataset as an input variable as well as a column name for a column in the dataset that contains binary (0 for negative and 1 for positive) values that you will summarize

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://www.learndatasci.com/glossary/binary-classification/</li>
<li>https://developers.google.com/machine-learning/crash-course/framing/ml-terminology</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>label_col&nbsp;– a string containing the name of the&nbsp;label&nbsp;column</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
<ul>
<li>n_records&nbsp;(int) – the number of rows in the dataset</li>
<li>n_columns&nbsp;(int) – the number of columns in the dataset</li>
<li>n_negative&nbsp;(int) – the number of “negative” samples in the dataset (label&nbsp;column equals 0)</li>
<li>n_positive&nbsp;(int) – the number of “positive” samples in the dataset (label&nbsp;column equals 1)</li>
<li>perc_positive&nbsp;(float) – the percentage (out of 100%) of positive samples in the dataset</li>
</ul>
<h5><strong>Function Skeleton</strong></h5>
import numpy as npimport pandas as pddef find_dataset_statistics(dataset:pd.DataFrame,label_col:str) -&gt; tuple[int,int,int,int,float]:&nbsp; n_records = <em>#TODO</em>&nbsp; n_columns = <em>#TODO</em>&nbsp; n_negative = <em>#TODO</em>&nbsp; n_positive = <em>#TODO</em>&nbsp; perc_positive = <em>#TODO</em>&nbsp; return n_records,n_columns,n_negative,n_positive,perc_positive

<h1>Task 2 (25 points)</h1>
Now that you have a basic understanding of pandas and the dataset it is time to dive into some more complex data processing tasks. These are basic concepts in model building but at a high level it is important to hold out a subset of your data when you train a model so you can see what the expected performance is on unseen samples and so you can determine if the resulting model is overfit (performs much better on training data vs test data). Preprocessing data is important since most models only take in numerical values so categorical features need to be “encoded” to numerical values so models can use them. Numerical scaling can be more or less useful depending on the type of model used but is especially important in linear models. These preprocessing techniques will provide you options to augment your dataset and improve model performance.

<h2>Useful Links:</h2>
<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/video-lecture">Training and Test Sets – Machine Learning – Google Developers</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias–variance tradeoff – Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting – Wikipedia</a></li>
<li><a href="https://365datascience.com/tutorials/statistics-tutorials/numerical-categorical-data/">Categorical and Numerical Types of Data – 365 Data Science</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn: machine learning in Python — scikit-learn 1.2.1 documentation</a></li>
</ul>
<h2>Deliverables:</h2>
<ul>
<li>Complete the functions and methods in task2.py</li>
<li>For this task we have released a local test suite please set that up and use it to debug your function.</li>
<li>Submit task2.py to Gradescape.</li>
</ul>
<h2>Instructions:</h2>
The Task2.py File has function skeletons that you will complete with python code (mostly using the pandas and scikit-learn libraries). The Goal of each of these functions is to give you familiarity with the applied concepts of Splitting and Preprocessing Data. See information about the Function’s Inputs, Outputs and Skeletons below

<h1>Table of contents</h1>
<ol>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#tts">tts</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#PreprocessDataset">PreprocessDataset</a>
<ol>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdataset__init__">__init__</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdatasetone_hot_encode_columns_train-and-one_hot_encode_columns_test">One Hot Encoding</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdatasetmin_max_scaled_columns_train-and-min_max_scaled_columns_test">Min/Max Scaling</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdatasetpca_train-and-pca_test">PCA</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdatasetfeature_engineering">Feature Engineering</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html#preprocessdatasetpreprocess_train-preprocess_test">Preprocess</a></li>
</ol>
</li>
</ol>
<h2>tts</h2>
In this function you will take a dataset, the name of its label column, a percentage of the data to put into the test set, if you should stratify on the label column and a random state to set the sklearn function with and you will return features and labels for the training and test sets. At a high level you can separate the task into 2 subtasks, first is splitting your dataset into both features and labels ( by columns) and second is splitting your dataset into training and test sets (by rows). You should use the sklearn train_test_split function but will have to write wrapper code around it based on the input values we give you.

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</li>
<li>https://developers.google.com/machine-learning/crash-course/framing/ml-terminology</li>
<li>https://stackoverflow.com/questions/40898019/what-is-the-difference-between-a-feature-and-a-label</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>dataset&nbsp;– a pandas dataframe that contains some data</li>
<li>label_col&nbsp;– a string containing the name of the column that contains the&nbsp;label&nbsp;values (what our model wants to predict)</li>
<li>test_size&nbsp;– a float containing the decimal value of the percentage of the number of rows that the test set should be out of the dataset</li>
<li>stratify&nbsp;– a boolean (True&nbsp;or&nbsp;False) value indicating if the resulting train/test split should be stratified or not</li>
<li>random_state&nbsp;– an integer value to set the randomness of the function (useful for repeatability especially when autograding)</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a pandas dataframe that contains the train rows and the feature columns</li>
<li>test_features&nbsp;– a pandas dataframe that contains the test rows and the feature columns</li>
<li>train_labels&nbsp;– a pandas dataframe that contains the train rows and the label column</li>
<li>test_labels&nbsp;– a pandas dataframe that contains the test rows and the label column</li>
</ul>
<h5><strong>Function Skeleton</strong></h5>
def tts(&nbsp; dataset: pd.DataFrame,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; label_col: str, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_size: float,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stratify: bool,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random_state: int) -&gt; tuple[pd.DataFrame,pd.DataFrame,pd.Series,pd.Series]:&nbsp;&nbsp;&nbsp; <em># TODO</em>&nbsp;&nbsp;&nbsp; return train_features,test_features,train_labels,test_labels

<h2>PreprocessDataset</h2>
The PreprocessDataset Class contains a code skeleton with 9 methods for you to implement. Most methods will be split into 2 parts: one that will be run on the training dataset and one that will be run on the test dataset. In Data Science/Machine Learning this is done in order to avoid something called&nbsp;<a href="https://machinelearningmastery.com/data-leakage-machine-learning/"><em>Data Leakage</em></a>. For this assignment, we don’t expect you to understand the nuances of the concept, but we will have you follow principles that will minimize the chances of it occurring. You will accomplish this by splitting data into training and test datasets and processing those datasets in slightly different ways. Generally for everything you do in this project and if you do any ML or Data Science work in the future you should train/fit on the training data first then predict/transform on the training and test data. That holds up for basic preprocessing steps like task2 and for complex models like you will see in tasks 3 and 4. For the purposes of this project you should&nbsp;<strong>never</strong>&nbsp;train or fit on the test data (and more generally in any ML project) because your test data is expected to give you an understanding of how your model/predictions will perform on unseen data and if you fit even a preprocessing step to your test data then you are either giving the model information about the test set it wouldnt have about unseen data (if you combine train and test and fit to both) or you are providing a different preprocessing than the model is expecting (if you fit a different preprocessor to the test data) and your model would not be expected to perform well.

<strong>Note</strong>: You should train/fit using the train dataset then once you have a fit encoder/scaler/pca/model instance you can transform/predict on the training and test data.

You will also notice that we are only preprocessing the Features and not the Labels. There are a few cases where preprocessing steps on labels may be helpful in modeling, but they are definitely more advanced and out of the scope of this introduction. Generally you will not need to do any preprocessing to your labels beyond potentially encoding a string value (ie “Malware” or “Benign”) into an integer value (0 or 1) which is called&nbsp;<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder"><em>Label Encoding</em></a>.

<h2>PreprocessDataset:__init__</h2>
Similar to the Task1 simpleClass subtask you previously completed you will initialize the class by adding instance variables (add all the inputs)

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://www.w3schools.com/python/python_classes.asp</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to tts which should be used in the training/fitting steps</li>
<li>test_features&nbsp;– a dataset split by a function similar to tts which should be used in the test steps</li>
<li>one_hot_encode_cols&nbsp;– a list of column names (strings) that should be one hot encoded by the one hot encode methods</li>
<li>min_max_scale_cols&nbsp;– a list of column names (strings) that should be min/max scaled by the min/max scaling methods</li>
<li>n_components&nbsp;– an int that contains the number of components that should be used in Principal Component Analysis</li>
<li>feature_engineering_functions&nbsp;– a dictionary that contains feature name and function to create that feature as a key value pair (example shown below)</li>
</ul>
Example of&nbsp;feature_engineering_functions:

def double_height(dataframe:pd.DataFrame):&nbsp;&nbsp;&nbsp; return dataframe[“height”] * 2def half_height(dataframe:pd.DataFrame):&nbsp;&nbsp;&nbsp; return dataframe[“height”] / 2feature_engineering_functions = {“double_height”:double_height,”half_height”:half_height}

Dont worry about copying it we also have examples in the local test cases this is just provided as an illustration of what to expect in your function.

<h5><strong>OUTPUTS</strong></h5>
None

<h5><strong>Function Skeleton</strong></h5>
def __init__(self, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_features:pd.DataFrame, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_features:pd.DataFrame,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; one_hot_encode_cols:list[str],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; min_max_scale_cols:list[str],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n_components:int,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_engineering_functions:dict&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em># TODO: Add any instance variables you may need to make your functions work</em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return

<h2>PreprocessDataset:one_hot_encode_columns_train&nbsp;and&nbsp;one_hot_encode_columns_test</h2>
One Hot Encoding is the process of taking a column and returning a binary vector representing the various values within it. There is a separate function for the training and test datasets since they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them).

<h3><strong>Pseudocode</strong></h3>
one_hot_encode_columns_train()

<ol>
<li>In the&nbsp;__init__()&nbsp;method initialize an instance variable containing an sklearn&nbsp;OneHotEncoder&nbsp;with any Parameters you may need</li>
<li>Split&nbsp;train_features&nbsp;into 2 dataframes: a dataframe with only the columns you want to one hot encode (using&nbsp;one_hot_encode_cols) and a dataframe with all the other columns</li>
<li>Fit the&nbsp;OneHotEncoder&nbsp;using the dataframe you split from&nbsp;train_features&nbsp;with the columns you want to encode</li>
<li>Transform the dataframe you split from&nbsp;train_features&nbsp;with the columns you want to encode using the fit&nbsp;OneHotEncoder</li>
<li>Create a Dataframe from the 2d array of data that the output from step 3 gave you, column names in the form of columnName_categoryName (there should be an attribute in&nbsp;OneHotEncoder&nbsp;that can help you with this) and the same index that&nbsp;train_features&nbsp;had</li>
<li>Join the dataframe you made in step 4 with the dataframe of other columns from step 1</li>
</ol>
one_hot_encode_columns_test()

<ol>
<li>Split&nbsp;test_features&nbsp;into 2 dataframes: a dataframe with only the columns you want to one hot encode (using&nbsp;one_hot_encode_cols) and a dataframe with all the other columns</li>
<li>Transform the dataframe you split from&nbsp;train_features&nbsp;with the columns you want to encode using the&nbsp;OneHotEncoder&nbsp;you fit in&nbsp;one_hot_encode_columns_train()</li>
<li>Create a Dataframe from the 2d array of data that the output from step 2 gave you, column names in the form of columnName_categoryName (there should be an attribute in&nbsp;OneHotEncoder&nbsp;that can help you with this) and the same index that&nbsp;test_features&nbsp;had</li>
<li>Join the dataframe you made in step 4 with the dataframe of other columns from step 1</li>
</ol>
<h3><strong>Example Walkthrough (from Local Testing suite):</strong></h3>
<h4><strong>INPUTS:</strong></h4>
<h5><strong>one_hot_encode_cols</strong></h5>
[“color”,”version”]

<h5><strong>Train Features</strong></h5>
<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color</strong></td>
<td><strong>version</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>0</td>
<td>red</td>
<td>1</td>
<td>5.99</td>
<td>12</td>
</tr>
<tr>
<td>6</td>
<td>yellow</td>
<td>6</td>
<td>10.99</td>
<td>18</td>
</tr>
<tr>
<td>3</td>
<td>red</td>
<td>1</td>
<td>5.99</td>
<td>15</td>
</tr>
<tr>
<td>9</td>
<td>red</td>
<td>8</td>
<td>12.99</td>
<td>21</td>
</tr>
<tr>
<td>2</td>
<td>blue</td>
<td>3</td>
<td>5.99</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>orange</td>
<td>5</td>
<td>10.99</td>
<td>17</td>
</tr>
<tr>
<td>1</td>
<td>green</td>
<td>2</td>
<td>5.99</td>
<td>13</td>
</tr>
<tr>
<td>7</td>
<td>green</td>
<td>2</td>
<td>12.99</td>
<td>19</td>
</tr>
</tbody>
</table>
<h5><strong>Test Features</strong></h5>
<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color</strong></td>
<td><strong>version</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>4</td>
<td>purple</td>
<td>4</td>
<td>10.99</td>
<td>16</td>
</tr>
<tr>
<td>8</td>
<td>blue</td>
<td>3</td>
<td>12.99</td>
<td>20</td>
</tr>
</tbody>
</table>
<h4><strong>TRAIN DATAFRAMES AT EACH STEP:</strong></h4>
<h5><strong>1.</strong></h5>
Dataframe with columns to encode:

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color</strong></td>
<td><strong>version</strong></td>
</tr>
<tr>
<td>0</td>
<td>red</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>yellow</td>
<td>6</td>
</tr>
<tr>
<td>3</td>
<td>red</td>
<td>1</td>
</tr>
<tr>
<td>9</td>
<td>red</td>
<td>8</td>
</tr>
<tr>
<td>2</td>
<td>blue</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>orange</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>green</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>green</td>
<td>2</td>
</tr>
</tbody>
</table>
Dataframe with other columns:

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>1</td>
<td>5.99</td>
<td>12</td>
</tr>
<tr>
<td>6</td>
<td>10.99</td>
<td>18</td>
</tr>
<tr>
<td>1</td>
<td>5.99</td>
<td>15</td>
</tr>
<tr>
<td>8</td>
<td>12.99</td>
<td>21</td>
</tr>
<tr>
<td>3</td>
<td>5.99</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>10.99</td>
<td>17</td>
</tr>
<tr>
<td>2</td>
<td>5.99</td>
<td>13</td>
</tr>
<tr>
<td>2</td>
<td>12.99</td>
<td>19</td>
</tr>
</tbody>
</table>
<h5><strong>3.</strong></h5>
One Hot Encoded 2d array:

<table>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h5><strong>4.</strong></h5>
One Hot Encoded Dataframe with Index and Column Names

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color_blue</strong></td>
<td><strong>color_green</strong></td>
<td><strong>color_orange</strong></td>
<td><strong>color_red</strong></td>
<td><strong>color_yellow</strong></td>
<td><strong>version_1</strong></td>
<td><strong>version_2</strong></td>
<td><strong>version_3</strong></td>
<td><strong>version_5</strong></td>
<td><strong>version_6</strong></td>
<td><strong>version_8</strong></td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>7</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h5><strong>5.</strong></h5>
Final Dataframe with passthrough columns joined back

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color_blue</strong></td>
<td><strong>color_green</strong></td>
<td><strong>color_orange</strong></td>
<td><strong>color_red</strong></td>
<td><strong>color_yellow</strong></td>
<td><strong>version_1</strong></td>
<td><strong>version_2</strong></td>
<td><strong>version_3</strong></td>
<td><strong>version_5</strong></td>
<td><strong>version_6</strong></td>
<td><strong>version_8</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5.99</td>
<td>12</td>
</tr>
<tr>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>10.99</td>
<td>18</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5.99</td>
<td>15</td>
</tr>
<tr>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>12.99</td>
<td>21</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5.99</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>10.99</td>
<td>17</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5.99</td>
<td>13</td>
</tr>
<tr>
<td>7</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>12.99</td>
<td>19</td>
</tr>
</tbody>
</table>
<h4><strong>TEST DATAFRAMES AT EACH STEP:</strong></h4>
<h5><strong>1.</strong></h5>
Dataframe with columns to encode:

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color</strong></td>
<td><strong>version</strong></td>
</tr>
<tr>
<td>4</td>
<td>purple</td>
<td>4</td>
</tr>
<tr>
<td>8</td>
<td>blue</td>
<td>3</td>
</tr>
</tbody>
</table>
Dataframe with other columns:

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>4</td>
<td>10.99</td>
<td>16</td>
</tr>
<tr>
<td>8</td>
<td>12.99</td>
<td>20</td>
</tr>
</tbody>
</table>
<h5><strong>2.</strong></h5>
One Hot Encoded 2d array:

<table>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h5><strong>3.</strong></h5>
One Hot Encoded Dataframe with Index and Column Names

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color_blue</strong></td>
<td><strong>color_green</strong></td>
<td><strong>color_orange</strong></td>
<td><strong>color_red</strong></td>
<td><strong>color_yellow</strong></td>
<td><strong>version_1</strong></td>
<td><strong>version_2</strong></td>
<td><strong>version_3</strong></td>
<td><strong>version_5</strong></td>
<td><strong>version_6</strong></td>
<td><strong>version_8</strong></td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h5><strong>4.</strong></h5>
Final Dataframe with passthrough columns joined back

<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>color_blue</strong></td>
<td><strong>color_green</strong></td>
<td><strong>color_orange</strong></td>
<td><strong>color_red</strong></td>
<td><strong>color_yellow</strong></td>
<td><strong>version_1</strong></td>
<td><strong>version_2</strong></td>
<td><strong>version_3</strong></td>
<td><strong>version_5</strong></td>
<td><strong>version_6</strong></td>
<td><strong>version_8</strong></td>
<td><strong>cost</strong></td>
<td><strong>height</strong></td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>10.99</td>
<td>16</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>12.99</td>
<td>20</td>
</tr>
</tbody>
</table>
<strong>Note:</strong>&nbsp;For the autograder use the column naming scheme of joining the previous column name and the column value with an underscore (similar to above where Type -&gt; Type_Fruit and Type_Vegtable)

<strong>Note 2:</strong>&nbsp;Since you should only be fitting your encoder on the training data if there are values in your test set that are different than those in the training set you will denote that with 0s. In the example above lets say we have a row in the test set with pizza which is neither a fruit or vegtable for the Type_Fruit and Type_Vegtable it should result in a 0 for both columns. If you dont handle these properly you may get errors like&nbsp;Test Failed: Found unknown categories.

<strong>Note 3:</strong>&nbsp;You may be tempted to use the pandas function&nbsp;get_dummies&nbsp;to solve this task but its a trap. It seems easier but you will have to do a lot more work to make it handle a train/test split so we suggest you use sklearn’s&nbsp;OneHotEncoder

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://www.educative.io/blog/one-hot-encoding</li>
<li>https://developers.google.com/machine-learning/data-prep/transform/transform-categorical</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder</li>
<li>https://datascience.stackexchange.com/questions/103211/do-we-need-to-pre-process-both-the-test-and-train-data-set</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the columns listed in&nbsp;one_hot_encode_cols&nbsp;one hot encoded and all other columns in the dataframe unchanged

<h5><strong>Function Skeleton</strong></h5>
def one_hot_encode_columns_train(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_datasetdef one_hot_encode_columns_test(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_dataset

<h2>PreprocessDataset:min_max_scaled_columns_train&nbsp;and&nbsp;min_max_scaled_columns_test</h2>
Min/Max Scaling is a process of scaling ints/floats from a min and max value in a series to between 0 and 1. The function for how scikit-learn does this is shown below but for this assignment you should just use the linked scikit-learn function.

X_std = (X – X.min(axis=0)) / (X.max(axis=0) – X.min(axis=0))X_scaled = X_std * (max – min) + min

There is a separate function for the training and test datasets since they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them).

Example Dataframe:

<table>
<tbody>
<tr>
<td><strong>Item</strong></td>
<td><strong>Price</strong></td>
<td><strong>Count</strong></td>
<td><strong>Type</strong></td>
</tr>
<tr>
<td>Apples</td>
<td>1.99</td>
<td>7</td>
<td>Fruit</td>
</tr>
<tr>
<td>Broccoli</td>
<td>1.29</td>
<td>435</td>
<td>Vegtable</td>
</tr>
<tr>
<td>Bananas</td>
<td>0.99</td>
<td>123</td>
<td>Fruit</td>
</tr>
<tr>
<td>Oranges</td>
<td>2.79</td>
<td>25</td>
<td>Fruit</td>
</tr>
<tr>
<td>Pineapples</td>
<td>4.89</td>
<td>5234</td>
<td>Fruit</td>
</tr>
</tbody>
</table>
Example One Hot Encoded Dataframe (rounded to 4 decimal places):

<table>
<tbody>
<tr>
<td><strong>Item</strong></td>
<td><strong>Price</strong></td>
<td><strong>Count</strong></td>
<td><strong>Type</strong></td>
</tr>
<tr>
<td>Apples</td>
<td>0.2564</td>
<td>7</td>
<td>Fruit</td>
</tr>
<tr>
<td>Broccoli</td>
<td>0.0769</td>
<td>435</td>
<td>Vegtable</td>
</tr>
<tr>
<td>Bananas</td>
<td>0</td>
<td>123</td>
<td>Fruit</td>
</tr>
<tr>
<td>Oranges</td>
<td>0.4615</td>
<td>25</td>
<td>Fruit</td>
</tr>
<tr>
<td>Pineapples</td>
<td>1</td>
<td>5234</td>
<td>Fruit</td>
</tr>
</tbody>
</table>
<strong>Note:</strong>&nbsp;For the autograder use the same name as the original column (ex: Price -&gt; Price)

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://developers.google.com/machine-learning/data-prep/transform/normalization</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler</li>
<li>https://datascience.stackexchange.com/questions/103211/do-we-need-to-pre-process-both-the-test-and-train-data-set</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the columns listed in&nbsp;min_max_scale_cols&nbsp;min/max scaled and all other columns in the dataframe unchanged

<h5><strong>Function Skeleton</strong></h5>
def min_max_scaled_columns_train(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; min_max_scaled_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return min_max_scaled_dataset&nbsp;&nbsp;&nbsp; def min_max_scaled_columns_test(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; min_max_scaled_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return min_max_scaled_dataset

<h2>PreprocessDataset:pca_train&nbsp;and&nbsp;pca_test</h2>
Principal Component analysis is a dimensionality reduction technique (column reduction). It aims to take the variance in your input columns and map the columns into N columns that contain as much of the variance as it can. This technique can be useful if you are trying to train a model faster and has some more advanced uses especially when training models on data which has many columns but few rows. There is a separate function for the training and test datasets since they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them).

<strong>Note:</strong>&nbsp;For the autograder use the column naming scheme of column names component_1, component_2 .. component_n for&nbsp;n_components&nbsp;passed into the&nbsp;__init__&nbsp;method

<strong>Note 2:</strong>&nbsp;For your PCA outputs to match the autograder make sure you set the seed using a random state of 0 when you initialize the PCA function.

<strong>Note 3:</strong>&nbsp;Since PCA does not work with NA values make sure you drop any columns that have NA values before running PCA

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://builtin.com/data-science/step-step-explanation-principal-component-analysis</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA</li>
<li>https://datascience.stackexchange.com/questions/103211/do-we-need-to-pre-process-both-the-test-and-train-data-set</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the generated pca values and using column names: component_1, component_2 .. component_n

<h5><strong>Function Skeleton</strong></h5>
def one_hot_encode_columns_train(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_datasetdef one_hot_encode_columns_test(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_dataset

<h2>PreprocessDataset:feature_engineering</h2>
Feature Engineering is a process of using domain knowledge (physics, geometry, sports statistics, business metrics, etc) to create new features (columns) out of the existing data. This could mean creating an area feature when given the length and width of a triangle or extracting the major and minor version number from a software version or more complex logic depending on the scenario. For this method you will be taking in a dictionary with a column name and a function (that takes in a dataframe and returns a column) and using that to create a new column with the name in the dict key.

For example:

def double_height(dataframe:pd.DataFrame):&nbsp;&nbsp;&nbsp; return dataframe[“height”] * 2def half_height(dataframe:pd.DataFrame):&nbsp;&nbsp;&nbsp; return dataframe[“height”] / 2feature_engineering_functions = {“double_height”:double_height,”half_height”:half_height}

with the above functions you would create 2 new columns named “double_height” and “half_height”.

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://en.wikipedia.org/wiki/Feature_engineering</li>
<li>https://www.geeksforgeeks.org/what-is-feature-engineering/</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the features described in&nbsp;feature_engineering_functions&nbsp;added as new columns and all other columns in the dataframe unchanged

<h5><strong>Function Skeleton</strong></h5>
def one_hot_encode_columns_train(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_datasetdef one_hot_encode_columns_test(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp; one_hot_encoded_dataset = pd.DataFrame()&nbsp;&nbsp;&nbsp; return one_hot_encoded_dataset

<h2>PreprocessDataset:preprocess_train,&nbsp;preprocess_test</h2>
Now we will put 3 of the above methods together into a preprocess function which will take in a dataset and encode, scale and feature engineer using the above methods and their respective columns and output a preprocessed dataframe.

<h5><strong>Useful Resources</strong></h5>
See resources for one hot encoding, min/max scaling and feature engineering above

<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe for both test and train features with the columns in&nbsp;one_hot_encode_cols&nbsp;encoded, the columns in&nbsp;min_max_scale_cols&nbsp;scaled and the columns described in&nbsp;feature_engineering_functions&nbsp;engineered.

<h5><strong>Function Skeleton</strong></h5>
def preprocess(self) -&gt; tuple[pd.DataFrame,pd.DataFrame]:&nbsp; train_features = pd.DataFrame()&nbsp; test_features = pd.DataFrame()&nbsp; return train_features,test_features

<h1>Task 3 (15 points)</h1>
So far we have functions to split the data and preprocess it. Now we will run a basic model on the data to cluster files (rows) with similar attributes together. We will use an unsupervised (model with no label column) model, Kmeans, since it is simple to use and understand. Please use scikit-learn to create the model and Yellowbrick to determine the optimal value of k for our dataset.

<h2>Useful Links:</h2>
<ul>
<li><a href="https://developers.google.com/machine-learning/clustering/overview">Clustering – Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/clustering/clustering-algorithms">Clustering Algorithms – Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/glossary#k-means">Kmeans – Google Developers</a></li>
</ul>
<h2>Deliverables:</h2>
<ul>
<li>Complete the KmeansClustering class in task3.py</li>
<li>For this task we have released a local test suite please set that up and use it to debug your function.</li>
<li>Submit task3.py to Gradescope</li>
</ul>
<h2>Instructions:</h2>
The Task3.py File has function skeletons that you will complete with python code (mostly using the pandas and scikit-learn libraries). The Goal of each of these functions is to give you familiarity with the applied concepts of Unsupervised Learning. See information about the Function’s Inputs, Outputs and Skeletons below

<h2>KmeansClustering</h2>
The KmeansClustering Class contains a code skeleton with 4 methods for you to implement.

<strong>Note</strong>: You should train/fit using the train dataset then once you have a fit encoder/scaler/pca/model instance you can transform/predict on the training and test data.

<h2>KmeansClustering:__init__</h2>
Similar to the Task1 simpleClass subtask you previously completed you will initialize the class by adding instance variables

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://www.w3schools.com/python/python_classes.asp</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to tts which should be used in the training/fitting steps</li>
<li>test_features&nbsp;– a dataset split by a function similar to tts which should be used in the test steps</li>
<li>random_state&nbsp;– an integer that should be used to set the scikit-learn randomness so the model results will be repeatable which is required for the autograder</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
None

<h5><strong>Function Skeleton</strong></h5>
def __init__(self, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_features:pd.DataFrame,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test_features:pd.DataFrame,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random_state: int&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ):&nbsp; <em># TODO: Add any state variables you may need to make your functions work</em>&nbsp; pass

<h2>KmeansClustering:kmeans_train</h2>
Kmeans Clustering is a process of grouping together similar rows together and assigning them to a cluster. For this method you will use the training data to fit an optimal kmeans cluster on the data.

To help you get started we have provided a list of subtasks to complete for this task:

<ol>
<li>Initialize a sklearn Kmeans model using random_state from the&nbsp;__init__&nbsp;method and setting n_init = 10.</li>
<li>Initialize a yellowbrick KElbowVisualizer to search for the optimal value of k (between 1 and 10).</li>
<li>Train the KElbowVisualizer on the training data and determine the optimal k value.</li>
<li>Train a Kmeans model with the proper initialization for that optimal value of k</li>
<li>Return the cluster ids for each row of the training set as a list.</li>
</ol>
<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans</li>
<li>https://www.scikit-yb.org/en/latest/api/cluster/elbow.html</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a list of cluster ids that the kmeans model has assigned for each row in the train dataset

<h5><strong>Function Skeleton</strong></h5>
def kmeans_train(self) -&gt; list:&nbsp;&nbsp;&nbsp; cluster_ids = list()&nbsp;&nbsp;&nbsp; return cluster_ids

<h2>KmeansClustering:kmeans_test</h2>
Kmeans Clustering is a process of grouping together similar rows together and assigning them to a cluster. For this method you will use the training data to fit an optimal kmeans cluster on the data.

To help you get started we have provided a list of subtasks to complete for this task:

<ol>
<li>Use the model you trained in the kmeans_train method to generate cluster ids for each row of the test dataset</li>
<li>Return the cluster ids for each row of the test set as a list.</li>
</ol>
<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans</li>
<li>https://www.scikit-yb.org/en/latest/api/cluster/elbow.html</li>
</ul>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a list of cluster ids that the kmeans model has assigned for each row in the test dataset

<h5><strong>Function Skeleton</strong></h5>
def kmeans_test(self) -&gt; list:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cluster_ids = list()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return cluster_ids

<h2>KmeansClustering:train_add_kmeans_cluster_id_feature,&nbsp;test_add_kmeans_cluster_id_feature</h2>
Using the two methods you completed above (kmeans_train&nbsp;and&nbsp;kmeans_test) you will add a feature to the training and test dataframes. This is similar to the feature engineering method in Task2. To do this, use the outout of the methods (the list of cluster ids) from the corresponding method and add it as a new column (named&nbsp;kmeans_cluster_id) in the input dataframe, then return the full dataframe.

<h5><strong>Useful Resources</strong></h5>
<h5><strong>INPUTS</strong></h5>
Use the needed instance variables you set in the&nbsp;__init__&nbsp;method

<h5><strong>OUTPUTS</strong></h5>
a pandas dataframe with the&nbsp;kmeans_cluster_id&nbsp;added as a feature and all other input columns unchanged

<h5><strong>Function Skeleton</strong></h5>
def train_add_kmeans_cluster_id_feature(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_df = pd.DataFrame()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return output_df&nbsp;&nbsp;&nbsp; def test_add_kmeans_cluster_id_feature(self) -&gt; pd.DataFrame:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_df = pd.DataFrame()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return output_df

<h1>Task 4 (25 points)</h1>
Finally we are ready to try a few different supervised classification models. We have chosen a few commonly used models for you to use here but there are many options and in the real world specific algorithms may fit a specific dataset better. You also won’t be doing any hyperparameter tuning yet to better focus on writing the code. You will train a model using the training set, predict on the training/test sets and calculate performance metrics and return a ModelMetrics object and trained scikit-learn model from each model function. (Note: You should use RFE for determining feature importance of the logistic regression model but do NOT use RFE for random forest or gradient boosting models to determine feature importance please use their built in values for this)

<h2>Useful Links:</h2>
<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/video-lecture">Training and Test Sets – Machine Learning – Google Developers</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias–variance tradeoff – Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting – Wikipedia</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn: machine learning in Python — scikit-learn 1.2.1 documentation</a></li>
<li><a href="https://builtin.com/machine-learning/classification-machine-learning">An Introduction to Classification in Machine Learning – builtin</a></li>
<li><a href="https://www.datacamp.com/blog/classification-machine-learning">Classification in Machine Learning: An Introduction – DataCamp</a></li>
</ul>
<h2>Deliverables:</h2>
<ul>
<li>Complete the functions and methods in task4.py</li>
<li>For this task we have released a local test suite please set that up and use it to debug your function.</li>
<li>Submit task4.py to Gradescape.</li>
</ul>
<h2>Instructions:</h2>
The Task4.py File has function skeletons that you will complete with python code (mostly using the pandas and scikit-learn libraries). The Goal of each of these functions is to give you familiarity with the applied concepts of Training a Model, Using it to score records and Calculating Performance Metrics for it. See information about the Function’s Inputs, Outputs and Skeletons below

<h1>Table of contents</h1>
<ol>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html#ModelMetrics">ModelMetrics</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html#calculate_naive_metrics">calculate_naive_metrics</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html#calculate_logistic_regression_metrics">calculate_logistic_regression_metrics</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html#calculate_random_forest_metrics">calculate_random_forest_metrics</a></li>
<li><a href="https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html#calculate_gradient_boosting_metrics">calculate_gradient_boosting_metrics</a></li>
</ol>
<h2>ModelMetrics</h2>
In order to simplify the Autograding we have created a class that will hold the metrics and Feature importances for a given trained model. You do not have to add anything to this class but are expected to use it (put your training and test metric dictionaries and feature importance dataframes inside it for the autograder to handle).

<h2>calculate_naive_metrics</h2>
A Naive model is a very simple model/prediction that can help to frame how well any more sophisticated model is doing. Since the Naive Model is incredibly basic (often a constant result or a randomly selected result) we should expect that any more sophisticated model that we train should outperform it. If the Naive Model beats a trained model it can mean that addtional data (rows or columns) is needed in the dataset to improve the model or that the dataset doesn’t have a strong enough signal for the target we want to predict. In this function you will use the approach of a constant output Naive model. You will calculate 4 metrics (accuracy,recall,precision&nbsp;and&nbsp;fscore) for the training and test datasets for a given constant integer as your prediction (passed into the function as the variable&nbsp;naive_assumption).

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://machinelearningmastery.com/how-to-develop-and-evaluate-naive-classifier-strategies-using-probability/</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>train_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>naive_assumption&nbsp;– an integer that should be used as the result from the Naive model</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
A completed&nbsp;ModelMetrics&nbsp;object with a training and test metrics dictionary with each one of the metrics&nbsp;<strong>rounded to 4 decimal places</strong>

<h5><strong>Function Skeleton</strong></h5>
def calculate_naive_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, naive_assumption:int) -&gt; ModelMetrics:&nbsp;&nbsp;&nbsp; train_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; test_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; naive_metrics = ModelMetrics(“Naive”,train_metrics,test_metrics,None)&nbsp;&nbsp;&nbsp; return naive_metrics

<h2>calculate_logistic_regression_metrics</h2>
A Logistic Regression model is a simple and more explainable statistical model that can be used to estimate the probability of an event (log-odds). At a high level a Logistic Regression model uses data in the training set to estimate a column’s weight in a linear approximation function (conceptually this is similar to estimating&nbsp;m&nbsp;for each column in the line formula you probably know well:&nbsp;y = m*x + b). If you are interested in learning more you can read up on the Math behind how this works. For this project we are more focused on showing you how to apply these models, so you can just use the sklearn Logistic Regression model in your code. For this task use Sklearn’s Logistic Regression class to train a Logistic Regression model (initialized using the kwargs passed into the function), predict scores for training and test datasets and calculate 7 metrics (accuracy,recall,precision,&nbsp;fscore,&nbsp;false positive rate (fpr),&nbsp;false negative rate (fnr)&nbsp;and&nbsp;Area Under the Curve of Reciever Operating Characteristics Curve (roc_auc)) for the training and test datasets using predictions from the fit model along with the a dataframe of the top 10 most important features.

<strong>NOTE:</strong>&nbsp;Make sure you use the predicted probabilities for roc auc

<strong>NOTE2:</strong>&nbsp;For Feature Importance use the top 10 features selected by RFE and sort by absolute value of the coefficient from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [Feature] and imp_col [Importance] and the index is reset to 0-9 you can do this the same way you did in task1)

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://stats.libretexts.org/Bookshelves/Introductory_Statistics/OpenIntro_Statistics_(Diez_et_al)./08%3A_Multiple_and_Logistic_Regression/8.04%3A_Introduction_to_Logistic_Regression</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score</li>
<li>https://en.wikipedia.org/wiki/Confusion_matrix</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>train_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>logreg_kwargs&nbsp;– a dictionary with keyword arguments that can be passed directly to the sklearn Logistic Regression class</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
<ul>
<li>A completed&nbsp;ModelMetrics&nbsp;object with a training and test metrics dictionary with each one of the metrics&nbsp;<strong>rounded to 4 decimal places</strong></li>
<li>An sklearn Logistic Regression model object fit on the training set</li>
</ul>
<h5><strong>Function Skeleton</strong></h5>
def calculate_logistic_regression_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, logreg_kwargs) -&gt; tuple[ModelMetrics,LogisticRegression]:&nbsp;&nbsp;&nbsp; model = LogisticRegression()&nbsp;&nbsp;&nbsp; train_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; test_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp; log_reg_importance = pd.DataFrame()&nbsp;&nbsp;&nbsp; log_reg_metrics = ModelMetrics(“Logistic Regression”,train_metrics,test_metrics,log_reg_importance)&nbsp;&nbsp;&nbsp;&nbsp; return log_reg_metrics,model

<h5><strong>Example of Feature Importance Dataframe</strong></h5>
<table>
<tbody>
<tr>
<td></td>
<td><strong>Feature</strong></td>
<td><strong>Importance</strong></td>
</tr>
<tr>
<td>0</td>
<td>density</td>
<td>-7.1416</td>
</tr>
<tr>
<td>1</td>
<td>volatile acidity</td>
<td>-6.6914</td>
</tr>
<tr>
<td>2</td>
<td>sulphates</td>
<td>1.4095</td>
</tr>
<tr>
<td>3</td>
<td>alcohol</td>
<td>1.0275</td>
</tr>
<tr>
<td>4</td>
<td>fixed acidity</td>
<td>-0.2234</td>
</tr>
<tr>
<td>5</td>
<td>pH</td>
<td>-0.1779</td>
</tr>
<tr>
<td>6</td>
<td>residual sugar</td>
<td>0.0683</td>
</tr>
<tr>
<td>7</td>
<td>free sulfur dioxide</td>
<td>0.0111</td>
</tr>
<tr>
<td>8</td>
<td>total sulfur dioxide</td>
<td>-0.0025</td>
</tr>
<tr>
<td>9</td>
<td>citric acid</td>
<td>0.0007</td>
</tr>
</tbody>
</table>
<h2>calculate_random_forest_metrics</h2>
A Random Forest model is a more complex model than the Naive and Logistic Regression Models you have trained so far. It can still be used to estimate the probability of an event but achieves this using a different underlying structure, A Tree Based Model. Conceptually this looks a lot like lots of if/else statements chained together into a “tree”. A Random Forest Expands on this and trains many different trees with different subsets of the data and starting conditions to get a better estimate than a single tree would give. For this project we are more focused on showing you how to apply these models, so you can just use the sklearn Random Forest model in your code. For this task use Sklearn’s Random Forest Classifier class to train a Random Forest model (initialized using the kwargs passed into the function), predict scores for training and test datasets and calculate 7 metrics (accuracy,recall,precision,&nbsp;fscore,&nbsp;false positive rate (fpr),&nbsp;false negative rate (fnr)&nbsp;and&nbsp;Area Under the Curve of Reciever Operating Characteristics Curve (roc_auc)) for the training and test datasets using predictions from the fit model along with the a dataframe of the top 10 most important features.

<strong>NOTE:</strong>&nbsp;Make sure you use the predicted probabilities for roc auc

<strong>NOTE2:</strong>&nbsp;For Feature Importance use the top 10 features selected by the built in method and sort by importance from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [Feature] and imp_col [Importance] and the index is reset to 0-9 you can do this the same way you did in task1)

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score</li>
<li>https://en.wikipedia.org/wiki/Confusion_matrix</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>train_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>rf_kwargs&nbsp;– a dictionary with keyword arguments that can be passed directly to the sklearn RandomForestClassifier class</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
<ul>
<li>A completed&nbsp;ModelMetrics&nbsp;object with a training and test metrics dictionary with each one of the metrics&nbsp;<strong>rounded to 4 decimal places</strong></li>
<li>An sklearn Random Forest model object fit on the training set</li>
</ul>
<h5><strong>Function Skeleton</strong></h5>
def calculate_random_forest_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, rf_kwargs) -&gt; tuple[ModelMetrics,RandomForestClassifier]:&nbsp;&nbsp;&nbsp; model = RandomForestClassifier()&nbsp;&nbsp;&nbsp; train_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; test_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp; rf_importance = pd.DataFrame()&nbsp;&nbsp;&nbsp; rf_metrics = ModelMetrics(“Random Forest”,train_metrics,test_metrics,rf_importance)&nbsp;&nbsp;&nbsp;&nbsp; return rf_metrics,model

<h5><strong>Example of Feature Importance Dataframe</strong></h5>
<table>
<tbody>
<tr>
<td></td>
<td><strong>Feature</strong></td>
<td><strong>Importance</strong></td>
</tr>
<tr>
<td>0</td>
<td>alcohol</td>
<td>0.3567</td>
</tr>
<tr>
<td>1</td>
<td>density</td>
<td>0.183</td>
</tr>
<tr>
<td>2</td>
<td>volatile acidity</td>
<td>0.1478</td>
</tr>
<tr>
<td>3</td>
<td>chlorides</td>
<td>0.0776</td>
</tr>
<tr>
<td>4</td>
<td>free sulfur dioxide</td>
<td>0.0725</td>
</tr>
<tr>
<td>5</td>
<td>citric acid</td>
<td>0.0684</td>
</tr>
<tr>
<td>6</td>
<td>total sulfur dioxide</td>
<td>0.0421</td>
</tr>
<tr>
<td>7</td>
<td>residual sugar</td>
<td>0.0187</td>
</tr>
<tr>
<td>8</td>
<td>fixed acidity</td>
<td>0.0144</td>
</tr>
<tr>
<td>9</td>
<td>pH</td>
<td>0.0097</td>
</tr>
</tbody>
</table>
<h2>calculate_gradient_boosting_metrics</h2>
A Gradient Boosted model is a more complex model than the Naive and Logistic Regression Models and similar in structure to the Random Forest Model you just trained. A Gradient Boosted Model Expands on the Tree Based Model by using its additional trees to predict the errors from the previous tree. For this project we are more focused on showing you how to apply these models, so you can just use the sklearn Gradient Boosted Model in your code. For this task use Sklearn’s Gradient Boosting Classifier class to train a Gradient Boosted model (initialized using the kwargs passed into the function), predict scores for training and test datasets and calculate 7 metrics (accuracy,recall,precision,&nbsp;fscore,&nbsp;false positive rate (fpr),&nbsp;false negative rate (fnr)&nbsp;and&nbsp;Area Under the Curve of Reciever Operating Characteristics Curve (roc_auc)) for the training and test datasets using predictions from the fit model along with the a dataframe of the top 10 most important features.

<strong>NOTE:</strong>&nbsp;Make sure you use the predicted probabilities for roc auc

<strong>NOTE2:</strong>&nbsp;For Feature Importance use the top 10 features selected by the built in method and sort by importance from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [Feature] and imp_col [Importance] and the index is reset to 0-9 you can do this the same way you did in task1)

<h5><strong>Useful Resources</strong></h5>
<ul>
<li>https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score</li>
<li>https://en.wikipedia.org/wiki/Confusion_matrix</li>
<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html</li>
</ul>
<h5><strong>INPUTS</strong></h5>
<ul>
<li>train_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_features&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>train_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>test_targets&nbsp;– a dataset split by a function similar to the tts function you created in task2</li>
<li>gb_kwargs&nbsp;– a dictionary with keyword arguments that can be passed directly to the sklearn GradientBoostingClassifier class</li>
</ul>
<h5><strong>OUTPUTS</strong></h5>
<ul>
<li>A completed&nbsp;ModelMetrics&nbsp;object with a training and test metrics dictionary with each one of the metrics&nbsp;<strong>rounded to 4 decimal places</strong></li>
<li>An sklearn Gradient Boosted model object fit on the training set</li>
</ul>
<h5><strong>Function Skeleton</strong></h5>
def calculate_random_forest_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, rf_kwargs) -&gt; tuple[ModelMetrics,RandomForestClassifier]:&nbsp;&nbsp;&nbsp; model = RandomForestClassifier()&nbsp;&nbsp;&nbsp; train_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; test_metrics = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “accuracy” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “recall” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “precision” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fscore” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fpr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “fnr” : 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “roc_auc” : 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp; rf_importance = pd.DataFrame()&nbsp;&nbsp;&nbsp; rf_metrics = ModelMetrics(“Random Forest”,train_metrics,test_metrics,rf_importance)&nbsp;&nbsp;&nbsp;&nbsp; return rf_metrics,model

<h5><strong>Example of Feature Importance Dataframe</strong></h5>
<table>
<tbody>
<tr>
<td></td>
<td><strong>Feature</strong></td>
<td><strong>Importance</strong></td>
</tr>
<tr>
<td>0</td>
<td>alcohol</td>
<td>0.3495</td>
</tr>
<tr>
<td>1</td>
<td>volatile acidity</td>
<td>0.2106</td>
</tr>
<tr>
<td>2</td>
<td>free sulfur dioxide</td>
<td>0.1077</td>
</tr>
<tr>
<td>3</td>
<td>residual sugar</td>
<td>0.0599</td>
</tr>
<tr>
<td>4</td>
<td>fixed acidity</td>
<td>0.0451</td>
</tr>
<tr>
<td>5</td>
<td>citric acid</td>
<td>0.045</td>
</tr>
<tr>
<td>6</td>
<td>total sulfur dioxide</td>
<td>0.0426</td>
</tr>
<tr>
<td>7</td>
<td>chlorides</td>
<td>0.0381</td>
</tr>
<tr>
<td>8</td>
<td>density</td>
<td>0.0367</td>
</tr>
<tr>
<td>9</td>
<td>sulphates</td>
<td>0.0326</td>
</tr>
</tbody>
</table>
<h1>Task 5 (20 points)</h1>
Now that you have written functions for different steps of the model building process you will put it all together. You will write code that trains a model with hyperparameters you determine (you should do any tuning locally or in a notebook ie don’t tune your model in gradescope since the autograder will likely timeout). It will take in the CLAMP training data (Note: the “class” column is the target for this dataset), train a model then predict on a test set (“class” column will be removed to simulate new files that will be classified by your model) and output values from 0 to 1 (values close to 0 being less likely to be malicious and closer to 1 being more likely to be malicious) for each row and our autograder will compare your predictions with the correct answers and to get credit you will need a roc auc score of .9 or higher on the test set (should not require much hyperparameter tuning for this dataset). This is basically a simulation of how your model would perform in the “production” system using batch inference.

<h2>Instructions:</h2>
<ul>
<li>Make use of any of the techniques we covered in this project to train a model and return predicted probabilities for each row of the test set as a DataFrame with columns&nbsp;index(same as your index from the input test df) and&nbsp;malware_score&nbsp;(predicted probabilities).</li>
<li>Complete the train_model_return_scores function in task5.py</li>
</ul>
<h3><strong>Sample Submission:</strong></h3>
<table>
<tbody>
<tr>
<td><strong>index</strong></td>
<td><strong>malware_score</strong></td>
</tr>
<tr>
<td>0</td>
<td>0.65</td>
</tr>
<tr>
<td>1</td>
<td>0.1</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<h2>Deliverables:</h2>
<ul>
<li>Submit task5.py to Gradescope</li>
</ul>
&nbsp;
